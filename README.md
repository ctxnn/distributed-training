in this repo, i will focus on distributed training with torch(maybe jax later) and try to scale it

i will start from training a simple neural network
- with only cpu
- with single gpu
- with double gpu

> i will use kaggle and colab gpus for this, because gpu poor :(

note: i have trained gpt2 with help of karpathy :), but i want to master training

todo:
- [ ] implement transformer with distributed training
